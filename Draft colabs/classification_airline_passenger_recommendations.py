# -*- coding: utf-8 -*-
"""Classification_Airline_Passenger_Recommendations.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AM4NUE8YmHPJVuKggB4OFrTPje7hPJBT

# **Project Name**    -

##### **Project Type**    - Classification
##### **Contribution**    - Team
##### **Team Member 1 -** Yash Kumar
##### **Team Member 2 -** Taniya Yadav
##### **Team Member 3 -** Vivek Mishra

# **Project Summary -**

The airline industry plays a crucial role in modern transportation, with numerous airlines serving various routes worldwide. To make informed decisions in this highly competitive industry, airlines and stakeholders rely on data-driven insights. Machine learning models are indispensable tools in this regard, allowing for the classification of airlines based on different criteria. This document outlines the development and implementation of an airline classification machine learning model.


This is where machine learning can play a vital role. By using historical customer data, a machine learning model can identify patterns and correlations that indicate a high likelihood of referral. This information can then be used by airlines to target specific customers with personalized marketing campaigns or incentives, increasing the chances of referral and promoting growth.


In conclusion, a machine learning model that predicts the likelihood of referral can provide valuable insights for airlines looking to enhance their customer satisfaction and drive growth through word-of-mouth referrals.
"""

from IPython.display import Image
Image(url='https://qph.cf2.quoracdn.net/main-qimg-445f62ae8197749e4c4114e6ab7fd4b8',
        width=1250,height=500)

"""# **GitHub Link -**

Provide your GitHub Link here.

# **Problem Statement**

The goal of this machine learning project is to classify airlines into categories based on certain features or attributes. Classification can serve multiple purposes, such as identifying potential partners for codeshare agreements, assisting in pricing strategies, or aiding in market analysis. In this project, We will be exploring if flyers would recommend the airline to their friends and families, based on their travel experience,reviews and ratings.

#### There are few problems that we are looking in this project:

* Develop a classification model to categorize airlines based on the likelihood of customers recommending them to friends and family.

* Recognize the pivotal role of customer satisfaction and referrals in the growth and success of airlines.

* Enable airlines to strategically utilize customer referral information for codeshare agreements, pricing strategies, and market analysis.

* Identify customers likely to refer the airline, a task complicated by the diverse factors influencing satisfaction and referrals.

* Assess the model's capability to provide actionable insights for airlines to tailor services, improve customer satisfaction, and enhance brand reputation.


This problem statement outlines the key objectives, challenges, and considerations for developing a classification model to predict customer referrals in the airline industry.
"""

from IPython.display import Image
Image(url='https://media.giphy.com/media/3ohc1fLZXOyhE4LOKc/giphy.gif?cid=790b76111m6jkxiorvqwjv0dmol4yn9pg3f8b0g0h0etfwz7&ep=v1_gifs_search&rid=giphy.gif&ct=g', width=1250,height=400)

"""# **General Guidelines** : -

1.   Well-structured, formatted, and commented code is required.
2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.
     
     The additional credits will have advantages over other students during Star Student selection.
       
             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go
                       without a single error logged. ]

3.   Each and every logic should have proper comments.
4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.
        

```
# Chart visualization code
```
            

*   Why did you pick the specific chart?
*   What is/are the insight(s) found from the chart?
* Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

5. You have to create at least 15 logical & meaningful charts having important insights.


[ Hints : - Do the Vizualization in  a structured way while following "UBM" Rule.

U - Univariate Analysis,

B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)

M - Multivariate Analysis
 ]





6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.


*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.


*   Cross- Validation & Hyperparameter Tuning

*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.

# ***Let's Begin !***

## ***1. Know Your Data***

### Import Libraries
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install category_encoders

# Import Libraries

import pandas as pd
import numpy as np
import seaborn as sns
import datetime as dt
import missingno as msno
import matplotlib.pyplot as plt
from scipy import stats
from scipy.stats import chi2_contingency
from scipy.stats import f_oneway
from sklearn.preprocessing import LabelEncoder
import category_encoders as ce
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.tree import DecisionTreeClassifier,plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score,accuracy_score,precision_score,recall_score,f1_score,confusion_matrix
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV , cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

"""### Dataset Loading"""

df1 =pd.read_excel('/content/drive/MyDrive/module_6_end_course/Dataset/data_airline_reviews.xlsx')

"""### Dataset First View"""

# Dataset First Look
df1.head()

"""### Dataset Rows & Columns count"""

# Dataset Rows & Columns count
df1.shape

"""### Dataset Information"""

# Dataset Info
df1.info()

"""#### Duplicate Values"""

# Dataset Duplicate Value Count
df1.duplicated().sum()

"""#### Missing Values/Null Values"""

# Missing Values/Null Values Count
df1.isnull().sum()

plt.figure(figsize=(15,8))
sns.heatmap(df1.isnull(), cbar=False, cmap='YlGnBu')
plt.title('Missing Values Heatmap')
plt.xticks(rotation=30)
plt.show()

"""### What did you know about your dataset?

Data includes airline reviews from 2006 to 2019 for popular airlines around the world with user feedback ratings and reviews based on their travel experience.

It has 131895 rows 17 different columns.

Data is scraped in Spring 2019. Feature descriptions briefly as follows:

1. **airline** - Airline name
2. **overall** - Overall score
3. **Author** - Author information
4. **review_date** - Customer Review posted date
5. **Customer_review** - Actual customer review(Textual)
6. **aircraft** - Type of aircraft
7. **traveller_type** - Type of traveller
8. **cabin**- Cabin type chosen by traveller (Economy, Business,Premium economy,First class)
9. **route** - Route flown by flyer
10. **date_flown** - Date of travel
11. **seat_comfort** - Rating provided towards seat comfort
12. **cabin_service** - Rating provided towards cabin service.
13. **food_bev** - Rating provided towards food and beverages supplied during travel.
14. **entertainment** - Rating provided towards on board flight entertainment
15. **ground_service** - Rating provided towards ground service staff.
16. **value_for_money** - Rating provided towards value for money.
17. **recommended** - Airline service Recommended by flyer (Yes/No)

## ***2. Understanding Your Variables***
"""

# Dataset Columns
df1.columns

# Dataset Describe
df1.describe().T

"""### Variables Description

It has lot of blank rows with many null values and the columns description are as follows:

1. **airline** - Name of the airline.  (**object type**)
2. **overall** - Overall rating defined by customer. (**float type**)
3. **Author** - Customer information. (**object type**)
4. **review_date** - date on which customer posted a review. (**object type**)
5. **Customer_review** - Description of customer review. (**object type**)
6. **aircraft** - Type of aircraft. (**object type**)
7. **traveller_type** - Type of traveller. (**object type**)
8. **cabin**- Cabin type chosen by traveller. (Economy, Business,Premium economy,First class) (**object type**)
9. **route** - Route flown by flyer. (**object type**)
10. **date_flown** - Date of travel. (**object type**)
11. **seat_comfort** - Rating provided towards seat comfort. (**float type**)
12. **cabin_service** - Rating provided towards cabin service. (**float type**)
13. **food_bev** - Rating provided towards food and beverages supplied during travel. (**float type**)
14. **entertainment** - Rating provided towards on board flight entertainment. (**float type**)
15. **ground_service** - Rating provided towards ground service staff. (**float type**)
16. **value_for_money** - Rating provided towards value for money. (**float type**)
17. **recommended** - Airline service Recommended by flyer (Yes/No). (**object type**)

### Check Unique Values for each variable.
"""

# Check Unique Values for each variable.
dict_uniq_value={}
dict_uniq_cnt={}
for i in df1.columns:
  dict_uniq_value[i]=df1[i].unique()
  dict_uniq_cnt[i]=len(df1[i].unique())

print(dict_uniq_value['airline'])
print(dict_uniq_cnt['airline'])

for i in df1.columns.tolist():
  print("No. of unique values in ",i,"is",df1[i].nunique())

"""## 3. ***Data Wrangling***

### Data Wrangling Code
"""

# Make a copy of your dataset for in future revert back
df=df1.copy()

#Drop all duplicated rows as there are many blank and duplicated rows
df.drop_duplicates(inplace=True)

#Drop the index column
df.reset_index(drop=True, inplace=True)

#Check for shape of your dataset
df.shape

#check for null values and sort in ascending order
df.isnull().sum().sort_values(ascending=False)

#Drop unwanted columns that is not used for our analysis
df.drop(columns=(['author','customer_review','route']),axis=1,inplace=True)

#In this we have too many null values so we dropped it and it can't be filled also
df.drop(columns=['aircraft'],axis=1,inplace=True)

# Again check for null values and sort in ascending order
df.isnull().sum().sort_values(ascending=False)

#Drop null values for these 2 columns
df.dropna(subset=(['ground_service','entertainment']),inplace=True)

# Again check for null values and sort in ascending order
df.isnull().sum().sort_values(ascending=False)

#Fill the null vales with mean fo their rating
df['food_bev'].fillna(df['food_bev'].mean(),inplace=True)

#Drop all null values in our whole dataset
df.dropna(inplace=True)

#Final check for null values
df.isnull().sum()

# Check for shape after cleaning or dataset
df.shape

#First row is all null values so after we dropped it our index starts from 1 so we are resetting or index
df.reset_index(drop=True, inplace=True)

#Check first 5 rows of dataset after cleaning
df.head()

"""### Outliars Detection and Removal"""

#Plot the boxplot for all columns to check for outliers
plt.figure(figsize=(12,8))
sns.boxplot(df)

"""## Data manipulation"""

#Check for Datatypes of columns in our dataset
df.info()

"""1. we can see there are many variables having not appropriate datatypes so we changed them to their suitable datatypes below."""

d_type={'overall':'int8','review_date':'datetime64[ns]','seat_comfort':'int8','cabin_service':'int8','food_bev':'int8','entertainment':'int8',
        'ground_service':'int8',
        'value_for_money':'int8'}
for i,j in d_type.items():
  df[i]=df[i].astype(j)

df.info()

"""Here we can see all columns are in their suitable datatype

2. Here converted `date_flown` column in a proper date format by removing timestamp and changed to Datetime format.
"""

df['date_flown']=pd.to_datetime(df['date_flown'], errors='coerce')

"""3. Renamed `overall` to `overall_rating` and `date_flown` to `departure_date` for better understanding."""

rename_col={'overall':'overall_rating','date_flown':'departure_date'}
df.rename(columns=rename_col,inplace=True)

df.head()

"""### What all manipulations have you done and insights you found?

1. Converted Date columns to datetime format as they were in object datatype and converted various rating columns from float to int as all ratings are only in integers.
2. date_flown column was not in proper date format it also contained Timestamp so we changed to a proper date format by removing timestamp and converted it into datetime datatype.
3. Renamed overall to overall_rating and date_flown to departure_date for better understanding.

# ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***

#### Chart - 1
"""

# Chart - 1 visualization code
plt.figure(figsize = (20,5))
air_cnt=df['airline'].value_counts().sort_values(ascending=False).head(10).reset_index()
palette = sns.color_palette("Set1", 10)
ax = sns.barplot(y=air_cnt['airline'],x=air_cnt['index'],palette =palette )
plt.xlabel('Airlines',fontsize=12)
plt.ylabel('Count of reviews',fontsize=12)
plt.title('Top 10 airlines based on highest trips',fontsize=18)
for num in ax.containers:
  ax.bar_label(num)
plt.show()

"""##### 1. Why did you pick the specific chart?

Bar graph is typically used when we have to depict categorical values with numerical values and here it suits well as we are to show airlines with its count of reviews

##### 2. What is/are the insight(s) found from the chart?

We have shown top 10 airlines in terms of their reviews count and can understand that American Airlines has the most number of review followed by United Airlines and Bristish Airways.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

It's essential to consider the nature of the reviews, the sentiments expressed, Understanding what customers appreciate about an airline, whether it's excellent service, punctuality, or other positive aspects, can help the company leverage and enhance these strengths and Identifying common issues or complaints allows the airline to address and rectify problems, leading to an improved customer experience.

#### Chart - 2
"""

cab_cnt=df['cabin'].value_counts().reset_index()

plt.figure(figsize=(12,6))
(plt.pie(cab_cnt['cabin'], labels=cab_cnt['index'], autopct='%1.1f%%', explode = [0, 0, 0.12, 0.2],startangle=60, textprops={'fontsize': 10},shadow=True,wedgeprops={'edgecolor': 'white'}))
plt.title('Distribution of Diffrent Cabin classes preffered by Passengers', y=1.08,fontsize = 12)
plt.show()

"""##### 1. Why did you pick the specific chart?

A pie chart is a circular statistical graphic that is divided into slices to illustrate numerical proportions. It's primarily used to show the relationship of parts to a whole.

##### 2. What is/are the insight(s) found from the chart?

From the above chart we can see that econony class constitues the largest part followed by business class and the other two class constitues very less portion of the chart which tells that mostly people were travelling in economy class .

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Understanding the popularity of economy class can guide the airline in customizing services to meet the needs and expectations of this larger customer segment,Given that economy class is the most popular, marketing efforts can be targeted towards this segment. Promotions, loyalty programs, and advertising can be tailored to attract and retain economy class travelers.

#### Chart - 3: `Bar Chart` for Comparing Most popular cabin type
"""

# Chart - 3 visualization code
most_trav = df["traveller_type"].value_counts().reset_index()
ax = sns.barplot(x=most_trav['index'],y = most_trav['traveller_type'] ,palette = 'Dark2')
plt.ylabel('Count')
plt.xlabel('Traveller type')
plt.title('Most Popular Travel type')
for num in ax.containers:
  ax.bar_label(num)
plt.show()

"""##### 1. Why did you pick the specific chart?

Bar graph is typically used when we have to depict categorical values with numerical values and here it suits well as we are to show air

##### 2. What is/are the insight(s) found from the chart?

`Solo Leisure` is the most preffered travel_type by passengers while `Bussiness` is the lowest travel_type.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Understanding that solo leisure travel is more popular allows the airline to tailor marketing efforts specifically toward this segment. Promotions, advertising, and loyalty programs can be designed to attract and retain solo leisure travelers, potentially increasing customer acquisition and retention.

#### Chart - 4: `Side by side Bar Chart` for comparing Cabin classes based on Food_bev and entertainment ratings

*  average ratings of Food_bev and entertainment given by passenger in various cabin
"""

#performing the grouphby method
eda_4=df.groupby('cabin')[['food_bev','entertainment']].mean().reset_index()
eda_4

plt.rcParams['figure.figsize']=(10,5)
eda_4.plot(x="cabin", y=["food_bev", "entertainment"], kind="bar")
plt.xticks(rotation=0)
plt.show()

"""##### 1. Why did you pick the specific chart?

This chart is best suited for showing side by side comparision of various cabin class wrt food_beverages and entertainment.

##### 2. What is/are the insight(s) found from the chart?

we can conclude that there is no significant change in ratings of `food_bev` and `entertainment` in Economy and first_class but in premium economy class there is more rating for entertainment as compared to food_bev and vice-versa for Bussiness_class.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Knowing that there are different preferences for entertainment and food_bev in Premium Economy and Business Class allows the airline to focus on enhancing services in each class selectively. This could involve improving menu options, upgrading entertainment systems, or introducing new features to align with passenger expectations

### Chart 5: Distribution of diffrent types of ratings using `Voilin Plot`
"""

columns = ['seat_comfort', 'cabin_service', 'food_bev', 'entertainment', 'ground_service', 'value_for_money','overall_rating']

# Melt the DataFrame to long format
df_melted = df.melt(value_vars=columns, var_name='Rating Category', value_name='Rating')

# Create a violin plot
plt.figure(figsize=(12, 6))
sns.violinplot(x='Rating Category', y='Rating', data=df_melted,palette='Set1')
plt.title('Violin Plot of various type of Ratings')
plt.xticks(rotation=45)
plt.show()

"""##### 1. Why did you pick the specific chart?

We chose `Voilin chart` because they are particularly useful for comparing the distribution of data between different groups or categories.This allows us to see not only the average rating for each type but also how ratings are spread out.

##### 2. What is/are the insight(s) found from the chart?

we can see that our mostly rating variables spreds between 1-6 and only `over_all rating` is ranging between 1-10 and mostly ratings are either lower range side or upper range side.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Airlines can use this insight to adjust pricing strategies.If customers rate the value for money lower, airlines can consider offering more competitive pricing or value-added services.hence improving specific aspects of service that are consistently rated lower can enhance the airline's brand reputation and differentiate it from competitors.

### Chart 6: Top Rated Airlines
"""

# Chart - 4 visualization code
plt.figure(figsize= (8,5))
overall = df.groupby(df['airline'])['overall_rating'].mean().sort_values(ascending = False).head(10).reset_index()
ax = sns.barplot(y=overall['airline'],x = overall['overall_rating'],palette ="tab10" )
plt.xlabel('Average overall rating')
plt.ylabel('Airline')
plt.title('Top rated airlines')

plt.show()

"""##### 1. Why did you pick the specific chart?

we pick horizontal column chart for comparison of various airlines wrt to Average overall rating.

##### 2. What is/are the insight(s) found from the chart?

from this chart we can see `Aegean airlines` is highest overall rating followed by EVA airlines and ANA all Nippon Airways while `Cathay Pecific airways` has the lowest rating .

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Aegean Airlines, EVA Airlines, and ANA All Nippon Airways can continue to focus on providing excellent service to maintain their high ratings. This can include improving in-flight amenities, on-time performance, and customer service.
 Cathay Pacific Airways, being rated lower, can invest in training and development programs for its staff to enhance customer service and improve overall customer experience.

#### Chart - 7: Top 10 Airlines wrt to value for money
"""

# Chart - 5 visualization code
plt.figure(figsize= (20,5))
val = df.groupby(df['airline'])['value_for_money'].mean().sort_values(ascending = False).head(10).reset_index()
ax = sns.barplot(x=val['airline'],y = val['value_for_money'] ,palette = 'viridis')

plt.title('Top 10 Airlines wrt to value for money',fontsize = 18)
plt.show()

"""##### 1. Why did you pick the specific chart?

we pick Bar chart for comparison of various airlines wrt to Average rating of value for money.

##### 2. What is/are the insight(s) found from the chart?

we can see that `EVA Air ` is the highest rated followed by China Southern Airlines and Aegean Airlines  for value for money .

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

based on the above insightes EVA Air can offer loyalty programs or incentives for frequent flyers to encourage repeat business and enhance customer loyalty.

#### Chart - 8: Distribution of overall rating
"""

# Chart - 7 visualization
sns.histplot(df['overall_rating'], kde = True,bins =5,color='#4CBB17')
plt.title('Distribution of overall rating')
plt.show()

"""##### 1. Why did you pick the specific chart?

we chose Histogram for distribution of Overall rating.

##### 2. What is/are the insight(s) found from the chart?

we can conclude that most people have rated between either 1-2 or 8-10
it shows that passenger have either best or worst experience with airline.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Based on above insightes Airlines can focus on addressing the aspects that lead to extreme negative experiences, such as poor customer service, flight delays, or uncomfortable seating, to reduce the number of low ratings and engaging with customers who have provided extreme ratings (either low or high) can provide valuable feedback for improvement and allow airlines to address specific pain points or areas of excellence.

#### Chart - 9: Cabin Class Recommendation based on service ratings
"""

plt.figure(figsize=(15,5))
sns.boxplot(x=df['cabin'], y=df['cabin_service'], hue = df['recommended'])
plt.legend(loc='upper right')

"""##### 1. Why did you pick the specific chart?

we pick this type of Boxplot to show rating comparison between diffrent cabin classes.

##### 2. What is/are the insight(s) found from the chart?

we can see for every cabin class if the service rating is more then 3 then passenger is more likely to recommend that airline to others.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

By focusing on enhancing service quality across all cabin classes to ensure ratings exceed 3, airlines can improve overall customer satisfaction, leading to positive recommendations and repeat business

negative-impact from insights:
If service ratings for any cabin class consistently fall below 3, it could lead to negative word-of-mouth, lower customer satisfaction, and a decline in recommendations, which could result in a loss of customers and revenue.

#### Chart - 10 - Number of reviews over months
"""

# Chart - 9 visualization code

plt.figure(figsize= (14,6))
plt.title('Number of reviews over months')
df['month_name'] = df['review_date'].dt.strftime('%B')
df['month'] = df['review_date'].dt.month
df2 = df[['month_name','month']].value_counts().reset_index().sort_values(by = 'month')
df2.rename(columns={0:'count'},inplace = True)
ax = sns.barplot(x = df2['month_name'], y = df2['count'],palette = 'Dark2')
for num in ax.containers:
  ax.bar_label(num)

"""##### 1. Why did you pick the specific chart?

Here we are plotting in which month how many reviews are submitting so with the help of this we can check is there any pattern or relation of number of reviews with the month so the best suited chart is a bar chart.

##### 2. What is/are the insight(s) found from the chart?

We can analyze from this chart that january month is having a large number of reviews as compared to others follwed by july and august while in the month of may we have the least may be january is a holiday or vacation month so more number of travellers are there so reviews is also there or the staff is not properly managing the services due to this reviews are more because passenger traffic is more.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

For holiday months if we are having more passenger traffic so we should employ the temporary staff to not spoil our services and management if the traffic is the reason.

#### Chart - 11 - Overall rating vs cabin type
"""

#Cabin type and overall service ratings (out of 10)
plt.figure(figsize=(10,5))
plt.title('Overall rating vs cabin type')
sns.barplot(x = df.cabin, y = df.overall_rating, hue = df['recommended'], palette= ['#00e500','red'])

"""##### 1. Why did you pick the specific chart?

Since we are plotting our categorical value against discrete numerical value so best suited chart is a side bar chart.

##### 2. What is/are the insight(s) found from the chart?

We can clearly see from this that almost in all cabin type we have overall rating more than 8 and the customer recommend the airline to others while for no we have almost 2 rating overall in economy while 3 for rest of all, so there is no much difference between cabin type if we see recommend yes/no .

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

We can say from above insights that if a person give overall rating more than 8 then its 99% sure that he is gonna recommend the airline to others by the help of rating we can request our customer to share their opinions on airline service on some platform for recommendation, while if person is not satisfied we will try to resolve their issue with best possible solution.

#### Chart - 12 - Correlation Heatmap
"""

# Correlation Heatmap visualization code
plt.figure(figsize=(8,5))
plt.title('Correlation Heatmap')
sns.heatmap(df.corr(), annot = True, fmt='.2f', annot_kws={'size': 10},  vmax=1, square=True,cmap="rocket")

"""##### 1. Why did you pick the specific chart?

This particular graph is the most powerful visualisation as it depicts the relationship of all the columns with each other and one another too.

##### 2. What is/are the insight(s) found from the chart?

Here on the graph the positive values tell us about that that particular variable is directly proportional to other one corresponding to it , the negative value indicates that the variable is indirectly proportional to corresponding varibale and larger the magnitude , more is the dependency. Also 1 indicates that if you were looking at a heatmap of a variable like value_for_money" plotted on both axes, and you saw a "1" in a cell, it would mean that when the minimum nights is high on the X-axis, it's also high on the Y-axis. When it's low on the X-axis, it's low on the Y-axis, and the relationship between these two instances of value_for_money is very strong and positive.

#### Chart - 13 - Pair Plot
"""

# Pair Plot visualization code
column_name = [ 'seat_comfort','value_for_money','cabin_service','ground_service']
pairplot_data = df[column_name]
chart15=sns.pairplot(pairplot_data,kind = 'reg')
plt.show()

"""##### 1. Why did you pick the specific chart?

Here we wanted to have a pairwise visualisation of all the columns in the dataset , hence used pairplot.

##### 2. What is/are the insight(s) found from the chart?

Since we have a discrete type of dataset so the distribution is showing here is not so perfect but by this we can see a positive relationship between all rating related columns.

# ***5. Hypothesis Testing***

### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing.

Answer Here.

### Hypothetical Statement - 1

#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.

* Null Hypothesis (H0): The overall ratings for two specific airlines are equal.


* Alternative Hypothesis (H1): The overall ratings for two specific airlines are not equal.

#### 2. Perform an appropriate statistical test.
"""

# Perform Statistical Test to obtain P-Value

# Calculate the means and standard deviations of the two halves
mean1 = (df[df['airline']=='Qatar Airways']['overall_rating']).mean()
mean2 = (df[df['airline']=='EVA Air']['overall_rating']).mean()
std1 = (df[df['airline']=='Qatar Airways']['overall_rating']).std()
std2 = (df[df['airline']=='EVA Air']['overall_rating']).std()

# Calculate the sample sizes
n1 = (df[df['airline']=='Qatar Airways']['overall_rating']).count()
n2 = (df[df['airline']=='EVA Air']['overall_rating']).count()

#Calculate the standard error for each airline
se1 = std1 / np.sqrt(n1)
se2 = std2 / np.sqrt(n2)

# Calculate the standard error of the difference between means
standard_error = np.sqrt(se1**2 + se2**2)

# Calculate the t_test
t_stat = (mean1 - mean2) / standard_error

#Significance level
alpha = 0.05

#Degree of freedom
dodf = n1 + n2 - 2

#calculating probability point function
cv = stats.t.ppf(1.0 - alpha, dodf)

# Calculate the p-value (two-tailed test)
p_value = (1 - stats.t.cdf(abs(t_stat), dodf)) * 2

# Set the significance level
alpha = 0.05

print('The p value for 0.05 significance level is {:.5f}'.format(p_value))

# Compare the p-value with the significance level
if p_value < alpha:
    print("Reject the null hypothesis. There is a significant difference in means.")
else:
    print("Fail to reject the null hypothesis. There is no significant difference in means.")

"""##### Which statistical test have you done to obtain P-Value?

We performed T-Test for this hypothesis testing to obtain P-value.

##### Why did you choose the specific statistical test?

We have a sample dataset and we are making inference about population and our population parameters are not known to us and to compare the overall ratings of the two selected airlines.

### Hypothetical Statement - 2

#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.

* Null Hypothesis (H0): There is no association between the traveller type and the likelihood of recommending the airline.


* Alternative Hypothesis (H1): There is an association between the traveller type and the likelihood of recommending the airline.

#### 2. Perform an appropriate statistical test.
"""

# Perform Statistical Test to obtain P-Value
# Create a contingency table
contingency_table = pd.crosstab(df['traveller_type'], df['recommended'])

# Perform chi-square test
chi2, p, dof, expected = chi2_contingency(contingency_table)

# Output the results
print(f'Chi-square statistic: {chi2}')
print(f'P-value: {p}')
print(f'Degrees of freedom: {dof}')


# Compare the p-value with the significance level
if p_value < alpha:
    print("Reject the null hypothesis. There is an association between the traveller type and the likelihood of recommending the airline.")
else:
    print("Fail to reject the null hypothesis. There is no association between the traveller type and the likelihood of recommending the airline.")

df.head()

"""##### Which statistical test have you done to obtain P-Value?

We performed Chi-square Test for this hypothesis testing to obtain P-value.

##### Why did you choose the specific statistical test?

A chi-square test of independence can be used to assess whether there is a significant relationship between the traveller type and the recommended status.

### Hypothetical Statement - 3

#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.

* Null Hypothesis (H0): The seat comfort ratings are the same across different cabin classes.


* Alternative Hypothesis (H1): There is a significant difference in seat comfort ratings among different cabin classes.

#### 2. Perform an appropriate statistical test.
"""

# Perform Statistical Test to obtain P-Value
# Perform one-way ANOVA
f_statistic, p_value = f_oneway(df['seat_comfort'][df['cabin'] == 'First Class'],
                                df['seat_comfort'][df['cabin'] == 'Business Class'],
                                df['seat_comfort'][df['cabin'] == 'Premium Economy'],
                                df['seat_comfort'][df['cabin'] == 'Economy Class'])

# Set significance level (alpha)
alpha = 0.05


print("\nResults:")
print(f"F-statistic: {f_statistic}")
print(f"P-value: ",p_value)

# Check for statistical significance
if p_value < alpha:
    print("\nResult: Reject the null hypothesis. There is a significant difference in seat comfort ratings.")
else:
    print("\nResult: Fail to reject the null hypothesis. No significant difference in seat comfort ratings.")

"""##### Which statistical test have you done to obtain P-Value?

We performed One-way ANOVA Test for this hypothesis testing to obtain P-value.

##### Why did you choose the specific statistical test?

A one way ANOVA test is used to compare the means of seat comfort ratings in different cabin classes.

# ***6. Feature Engineering & Data Pre-processing***

### 1. Handling Missing Values

We have already handled the missing values in exploratory data analysis.

#### What all missing value imputation techniques have you used and why did you use those techniques?

### 2. Handling Outliers

##### What all outlier treatment techniques have you used and why did you use those techniques?

The dataset had no outliers so there was no need of handling as such.

### 3. Categorical Encoding
"""

# Encode your categorical columns
label_encode = LabelEncoder()
df['recommended'] = label_encode.fit_transform(df['recommended'])

df['cabin'].unique()

ordinal_encoder = ce.OrdinalEncoder(mapping=[{'col': 'cabin', 'mapping': {'Economy Class': 1, 'Business Class': 3,'Premium Economy' : 2,'First Class' :4}}])
df['cabin']= ordinal_encoder.fit_transform(df['cabin'])

one_hot_encoder = ce.OneHotEncoder(cols=['traveller_type'])
df = one_hot_encoder.fit_transform(df)

"""#### What all categorical encoding techniques have you used & why did you use those techniques?

As we can not give categorical values in machine learning model so we need to encode them with numerical values . Wr have use different techniques of encoding for different columns


For the **"Traveller_Type"** column, which appears to represent categorical data with different types of travelers (e.g., Solo Leisure), it's appropriate to use one-hot encoding. One-hot encoding is commonly used for categorical variables with multiple levels, where each level is treated as a distinct category.

For **"Cabin" column** : There is a clear ordinal relationship, where the different cabin classes have a meaningful and consistent order (e.g., Economy < Premium Economy < Business < First Class), then ordinal encoding could be a suitable choice. In this case, each category is assigned a numerical value based on its order.

For **"recommended"** column : Since we have two categories like "Yes" and "No, we used label encoding. Label encoding involves assigning a numerical label to each unique category. For "Yes" and "No," you could encode them as 1 and 0, respectively.

### 4. Feature Manipulation & Selection

#### 1. Feature Manipulation

...........

#### 2. Feature Selection
"""

df[['overall_rating','seat_comfort','food_bev','cabin_service','entertainment','ground_service','value_for_money','recommended']].corr()

# Manipulate Features to minimize feature correlation and create new features
df.drop('overall_rating',axis = 1 ,inplace = True)

df.drop(columns = ['review_date','month_name','month','departure_date','airline'],inplace = True)

"""##### What all feature selection methods have you used  and why?

Feature selection is the process of choosing a subset of relevant features (variables, predictors) for use in model construction

We basically used **filter method** which evaluate the relevance of features based on statistical measures or scores calculated independently of the machine learning algorithm.

Here we used **Correlation coefficient** that Measures the linear relationship between two variables. Features with high correlation to the target variable or with other features may be considered redundants and so is the reason we dropped overall_rating column.

Columns like **'review_date'** and **'departure_date**' represent date or time-related information, which is not directly interpretable by most machine learning algorithms.

Additionally **'month'** and **'month_name'** are the columns that we made for analysis purpose so model do not require to get trained with these so dropping them too.

Column **'airline'** is basically the name of the airline which is again not relevant for the model to get trained on.

##### Which all features you found important and why?

Features like

### 5. Data Transformation
"""



"""#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"""

# Transform Your data

"""### 6. Data Scaling

Scaling data is used to standardize the range of independent variables or features. It's particularly useful when features have different scales or units of measurement, and here the features are on the same scale so there is no need of scaling.

### 7. Dimesionality Reduction

##### Do you think that dimensionality reduction is needed? Explain Why?

Answer Here.
"""

# DImensionality Reduction (If needed)
pca = PCA()
airline_pca = pca.fit_transform(df.iloc[:,:-1])

# Convert PCA components to a DataFrame for better readability
airline_pca_df = pd.DataFrame(data=airline_pca, columns=[f'PC{i+1}' for i in range(len(df.iloc[:,:-1].columns))])

# Display the first few rows of the PCA components
airline_pca_df.head()

explained_variance = pca.explained_variance_ratio_

# Plotting the explained variance
plt.figure(figsize=(10, 4))
op=sns.barplot(x=range(1, len(explained_variance) + 1), y=explained_variance*100, hue=range(1, len(explained_variance) + 1),palette='colorblind',legend=False)
plt.ylabel('Variance Percentage')
plt.xlabel('Principal Component')
plt.title('PCA Explained Variance Percentage')
for i, num in enumerate(explained_variance):
    op.text(i, num * 100+1, f'{num*100:.2f}',ha='center')
plt.ylim(0 , 90)
plt.xlim(-1 , 12)
plt.show()

pca.explained_variance_ratio_

for i in range(1,len(explained_variance)):
  print(f'Sum of percentage of variance of {i} columns',round(sum(explained_variance[0:i]*100),2))

pca_2 = PCA(n_components=6)
airline_pca_2 = pca_2.fit_transform(df.iloc[:,:-1])

# Shape after reduction
print("Original Shape:", df.iloc[:,:-1].shape)
print("Transformed Shape:", airline_pca_2.shape)

airline_pca_2_df = pd.DataFrame(data=airline_pca_2, columns=[f'PC{i+1}' for i in range(len(df.iloc[:,:6].columns))])

airline_pca_2_df.head(3)

"""##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)

Answer Here.

### 8. Data Splitting
"""

# Split your data to train and test. Choose Splitting ratio wisely.
x = airline_pca_2_df
y = df.iloc[:,-1]

x

y

"""##### What data splitting ratio have you used and why?

Answer Here.

### 9. Handling Imbalanced Dataset

##### Do you think the dataset is imbalanced? Explain Why.

Answer Here.
"""

# Handling Imbalanced Dataset (If needed)
y.value_counts()

"""##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)

Answer Here.

# ***7. ML Model Implementation***
"""

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

print('Shape of X_train',X_train.shape)
print('Shape of y_train',y_train.shape)
print('Shape of X_test',X_test.shape)
print('Shape of y_test',y_test.shape)

"""### ML Model - 1 - Decision Tree"""

# ML Model - 1 Implementation

model = DecisionTreeClassifier()

# Fit the Algorithm
model.fit(X_train, y_train)
# Predict on the model
prediction = model.predict(X_test)

print("Accuracy on test data:", accuracy_score(y_test, prediction))
print("Precision on test data:", precision_score(y_test, prediction))
print("Recall on test data:", recall_score(y_test, prediction))
print("F1_score on test data:", f1_score(y_test, prediction))

# ML Model - 1 Implementation

model1 = DecisionTreeClassifier()

# Fit the Algorithm
model1.fit(X_train, y_train)
# Predict on the model
prediction1 = model1.predict(X_train)

print("Accuracy on training data:", accuracy_score(y_train, prediction1))
print("Precision on training data:", precision_score(y_train, prediction1))
print("Recall on training data:", recall_score(y_train, prediction1))
print("F1_score on training data:", f1_score(y_train, prediction1))

cm = confusion_matrix(y_test, prediction)

cm

ax= plt.subplot()
labels = ['Recommended', 'Not Recommend']
sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels')
ax.set_ylabel('True labels')
ax.set_title('Confusion Matrix')
ax.xaxis.set_ticklabels(labels)
ax.yaxis.set_ticklabels(labels)

"""#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."""

# Visualizing evaluation Metric Score chart

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)
dt_classifier = DecisionTreeClassifier()

param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [2, 5, 10, 20, 30, 40],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
# Fit the Algorithm
grid_search = GridSearchCV(dt_classifier, param_grid, cv=5, scoring='f1')
grid_search.fit(X_train, y_train)

# Predict on the
best_params = grid_search.best_params_

best_params

best_dt_classifier = DecisionTreeClassifier(**best_params)
best_dt_classifier.fit(X_train, y_train)
prediction2 = best_dt_classifier.predict(X_test)

print("Accuracy on test data:", accuracy_score(y_test, prediction2))
print("Precision on test data:", precision_score(y_test, prediction2))
print("Recall on test data:", recall_score(y_test, prediction2))
print("F1_score on test data:", f1_score(y_test, prediction2))

best_dt_classifier1 = DecisionTreeClassifier(**best_params)
best_dt_classifier1.fit(X_train, y_train)
prediction3 = best_dt_classifier1.predict(X_train)

print("Accuracy on test data:", accuracy_score(y_train, prediction3))
print("Precision on test data:", precision_score(y_train, prediction3))
print("Recall on test data:", recall_score(y_train, prediction3))
print("F1_score on test data:", f1_score(y_train, prediction3))

"""##### Which hyperparameter optimization technique have you used and why?

Answer Here.

##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

Answer Here.

### ML Model - 2
"""

knn = KNeighborsClassifier()

# Train the classifier
knn.fit(X_train, y_train)

# Make predictions on the test set
y_pred_knn = knn.predict(X_test)

print("Accuracy on test data:", accuracy_score(y_test, y_pred_knn))
print("Precision on test data:", precision_score(y_test, y_pred_knn))
print("Recall on test data:", recall_score(y_test, y_pred_knn))
print("F1_score on test data:", f1_score(y_test,y_pred_knn))

knn1 = KNeighborsClassifier()

# Train the classifier
knn1.fit(X_train, y_train)

# Make predictions on the test set
y_pred_knn1 = knn1.predict(X_train)

print("Accuracy on training data:", accuracy_score(y_train, y_pred_knn1))
print("Precision on training data:", precision_score(y_train, y_pred_knn1))
print("Recall on training data:", recall_score(y_train, y_pred_knn1))
print("F1_score on training data:", f1_score(y_train,y_pred_knn1))

"""#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."""



"""#### 2. Cross- Validation & Hyperparameter Tuning"""

# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)

# Fit the Algorithm

# Predict on the model
param_grid = {
    'n_neighbors': [3, 5, 7, 9],  # Number of neighbors
    'weights': ['uniform', 'distance'],  # Weighting scheme
    'metric': ['euclidean', 'manhattan']  # Distance metric
}

# Instantiate the KNN classifier
knn = KNeighborsClassifier()

# Instantiate GridSearchCV
grid_search1 = GridSearchCV(knn, param_grid, cv=5, scoring='f1')

# Fit GridSearchCV to the training data
grid_search1.fit(X_train, y_train)

# Retrieve the best parameters and best score
best_params = grid_search1.best_params_
best_score = grid_search1.best_score_

print("Best Parameters:", best_params)
print("Best Score:", best_score)

# Evaluate the best model on the testing set
best_model = grid_search1.best_estimator_
test_score = best_model.score(X_test, y_test)
print("Test Set Score:", test_score)

curr_parm_knn = best_model.get_params()
curr_parm_knn

knn1_hy = KNeighborsClassifier(**curr_parm_knn)

# Train the classifier
knn1_hy.fit(X_train, y_train)

# Make predictions on the test set
y_pred_knn1_hy = knn1_hy.predict(X_test)

print("Accuracy on test data:", accuracy_score(y_test,y_pred_knn1_hy))
print("Precision on test data:", precision_score(y_test, y_pred_knn1_hy))
print("Recall on test data:", recall_score(y_test, y_pred_knn1_hy))
print("F1_score on test data:", f1_score(y_test,y_pred_knn1_hy))

knn1_hy1 = KNeighborsClassifier(**curr_parm_knn)

# Train the classifier
knn1_hy1.fit(X_train, y_train)

# Make predictions on the test set
y_pred_knn1_hy1 = knn1_hy1.predict(X_train)

print("Accuracy on training data:", accuracy_score(y_train, y_pred_knn1_hy1))
print("Precision on training data:", precision_score(y_train, y_pred_knn1_hy1))
print("Recall on training data:", recall_score(y_train, y_pred_knn1_hy1))
print("F1_score on training data:", f1_score(y_train,y_pred_knn1_hy1))

cfu = confusion_matrix(y_test, y_pred_knn)
ax= plt.subplot()
labels = ['Recommended', 'Not Recommend']
sns.heatmap(cfu, annot=True, ax = ax) #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels')
ax.set_ylabel('True labels')
ax.set_title('Confusion Matrix')
ax.xaxis.set_ticklabels(labels)
ax.yaxis.set_ticklabels(labels)

"""##### Which hyperparameter optimization technique have you used and why?

Answer Here.

##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

Answer Here.

#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used.

Answer Here.

### ML Model - 3
"""

# ML Model - 3 Implementation

# Fit the Algorithm

# Predict on the model
svm_model = SVC(kernel='rbf', gamma = 'auto', random_state=42)
svm_model.fit(X_train,y_train)
pred_svm = svm_model.predict(X_test)

print("Accuracy on test data:", accuracy_score(y_test, pred_svm))
print("Precision on test data:", precision_score(y_test, pred_svm))
print("Recall on test data:", recall_score(y_test, pred_svm))
print("F1_score on test data:", f1_score(y_test,pred_svm))

pred_svm1 = svm_model.predict(X_train)

print("Accuracy on training data:", accuracy_score(y_train, pred_svm1))
print("Precision on training data:", precision_score(y_train, pred_svm1))
print("Recall on training data:", recall_score(y_train, pred_svm1))
print("F1_score on training data:", f1_score(y_train,pred_svm1))

y_train.min()

"""#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."""

# Visualizing evaluation Metric Score chart

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Define the parameter grid to search
param_grid_svm = {'C': [0.1, 1, 10, 100],  # Regularization parameter
              'gamma': [0.001, 0.01, 0.1, 1],  # Kernel coefficient (only for RBF kernel)
              'kernel': ['linear', 'rbf']}  # Kernel type

# Create an SVM model
svm_model = SVC()

# Perform grid search with 5-fold cross-validation
random_search = RandomizedSearchCV(svm_model, param_grid_svm,verbose=2,random_state = 42 ,cv=5,scoring='f1')
random_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params_svm = random_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best SVM model
best_svm_model = random_search.best_estimator_

# Evaluate the best model on the test set
y_pred_svm = best_svm_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred_svm)
print("Accuracy:", accuracy)

curr_param_svm = best_svm_model.get_params()
curr_param_svm

svm_model = SVC(**curr_param_svm)
svm_model.fit(X_train,y_train)
pred_svm1 = svm_model.predict(X_test)

print("Accuracy on test data:", accuracy_score(y_test, pred_svm1))
print("Precision on test data:", precision_score(y_test, pred_svm1))
print("Recall on test data:", recall_score(y_test, pred_svm1))
print("F1_score on test data:", f1_score(y_test,pred_svm1))

svm_model1 = SVC(**curr_param_svm)
svm_model1.fit(X_train,y_train)
pred_svm2 = svm_model.predict(X_train)

print("Accuracy on training data:", accuracy_score(y_train, pred_svm2))
print("Precision on training data:", precision_score(y_train, pred_svm2))
print("Recall on training data:", recall_score(y_train, pred_svm2))
print("F1_score on training data:", f1_score(y_train,pred_svm2))

"""##### Which hyperparameter optimization technique have you used and why?

Answer Here.

##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

Answer Here.

##ML Model - 4
"""

rf = RandomForestClassifier()

# Fit the Algorithm
rf.fit(X_train,y_train)

# Predict on the model
y_pred_rf =rf.predict(X_test)

print("Accuracy on test data:", accuracy_score(y_test, y_pred_rf))
print("Precision on test data:", precision_score(y_test, y_pred_rf))
print("Recall on test data:", recall_score(y_test, y_pred_rf))
print("F1_score on test data:", f1_score(y_test,y_pred_rf))

rf1 = RandomForestClassifier()

# Fit the Algorithm
rf1.fit(X_train,y_train)

# Predict on the model
y_pred_rf1 =rf1.predict(X_train)

print("Accuracy on training data:", accuracy_score(y_train, y_pred_rf1))
print("Precision on training data:", precision_score(y_train, y_pred_rf1))
print("Recall on training data:", recall_score(y_train, y_pred_rf1))
print("F1_score on training data:", f1_score(y_train,y_pred_rf1))

param_grid_rf = {
    'n_estimators': [50,80,100,200,300],
    'max_depth': [1,2,6,7,8,9,10,20,30,40],
    'min_samples_split':[5,10,20,30,40,50,100,150,200],
    'min_samples_leaf': [1,2,8,10,20,40,50]
}

# Create the RandomizedSearchCV object
random_search = RandomizedSearchCV(rf, param_grid_rf,verbose=2,random_state = 42 ,cv=5,scoring='f1')

# Fit the RandomizedSearchCV object to the training data
random_search.fit(X_train, y_train)

# Get the best estimator
best_model_rf_rs = random_search.best_estimator_

best_model_rf_rs.feature_importances_
print(best_model_rf_rs)

curr_param = best_model_rf_rs.get_params()
curr_param

rf3 = RandomForestClassifier(**curr_param)

# Fit the Algorithm
rf3.fit(X_train,y_train)

# Predict on the model
y_pred_rf3 =rf3.predict(X_test)

print("Accuracy on test data:", accuracy_score(y_test, y_pred_rf3))
print("Precision on test data:", precision_score(y_test, y_pred_rf3))
print("Recall on test data:", recall_score(y_test, y_pred_rf3))
print("F1_score on test data:", f1_score(y_test,y_pred_rf3))

rf4 = RandomForestClassifier(**curr_param)

# Fit the Algorithm
rf4.fit(X_train,y_train)

# Predict on the model
y_pred_rf4 =rf4.predict(X_train)

print("Accuracy on training data:", accuracy_score(y_train, y_pred_rf4))
print("Precision on training data:", precision_score(y_train, y_pred_rf4))
print("Recall on training data:", recall_score(y_train, y_pred_rf4))
print("F1_score on training data:", f1_score(y_train,y_pred_rf4))

"""### 1. Which Evaluation metrics did you consider for a positive business impact and why?"""



"""Answer Here.

### 2. Which ML model did you choose from the above created models as your final prediction model and why?

Answer Here.

### 3. Explain the model which you have used and the feature importance using any model explainability tool?

Answer Here.

## ***8.*** ***Future Work (Optional)***

### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.
"""

# Save the File

"""### 2. Again Load the saved model file and try to predict unseen data for a sanity check.

"""

# Load the File and predict unseen data.

"""### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***

# **Conclusion**

Write the conclusion here.

### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***
"""